{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_honor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjF3CPsZSGHT",
        "colab_type": "text"
      },
      "source": [
        "Two approaches were considered for this project:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Conditional language modeling**: seq2seq model using encoder-attention-decoder architecture.\n",
        "2.   **Selective model**: using embeddings-based ranking\n",
        "\n",
        "\n",
        "In this notebook, the results of the selective approach are shown. The other approach is still experimental, although that is also promising. For that approach, please refer to the following github repo.\n",
        "\n",
        "[encoder-attention-decoder architecture](https://github.com/ragibhassantonoy/NLP_Honor/blob/master/attention_week5_honor.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQ-KnxIUagy",
        "colab_type": "text"
      },
      "source": [
        "The architecture:\n",
        "\n",
        "1.   Preprocess the dataset so that the model can also recognise the tone, beginning and end of the sentences.\n",
        "2.   Group sentences in pairs to construct conversations.\n",
        "3.   Learn unsupervised sentence embeddings on the corpus\n",
        "4. Select best (or, at random from multiple bests) response based on the input embedding.\n",
        "\n",
        "5. The codes can be found here\n",
        "[selective architecture](https://github.com/ragibhassantonoy/NLP_Honor/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CchEOtNRZ4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()  \n",
        "# setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_project()\n",
        "\n",
        "setup_google_colab.setup_honor()\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8yzz--TXeQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh6J_2iuWh1u",
        "colab_type": "text"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Four datasets were considered for training:\n",
        "\n",
        "1. [Cornell Movie Dialogs](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
        "2. [OpenSubs](http://opus.nlpl.eu/OpenSubtitles.php)\n",
        "3. Scotus\n",
        "4. Ubuntu dialogs\n",
        "\n",
        "\n",
        "The codes for mining and initial preprocessing are adapted from the following links:\n",
        "\n",
        "\n",
        "\n",
        "1.   [DeepQA](https://github.com/Conchylicultor/DeepQA#presentation)\n",
        "2.   [HSE github](https://github.com/hse-aml/natural-language-processing/tree/master/honor)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLbOpZRRRmp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "06706580-d86c-4119-b281-677087068605"
      },
      "source": [
        "# Copyright 2015 Conchylicultor. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "import ast\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from time import time\n",
        "\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import datetime\n",
        "import sys\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "from gzip import GzipFile\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "#from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "\"\"\"\n",
        "Load the cornell movie dialog corpus.\n",
        "\n",
        "Available from here:\n",
        "http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class CornellData:\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dirName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dirName (string): directory where to load the corpus\n",
        "        \"\"\"\n",
        "        self.lines = {}\n",
        "        self.conversations = []\n",
        "\n",
        "        MOVIE_LINES_FIELDS = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
        "        MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
        "\n",
        "        self.lines = self.loadLines(os.path.join(dirName, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
        "        self.conversations = self.loadConversations(os.path.join(dirName, \"movie_conversations.txt\"), MOVIE_CONVERSATIONS_FIELDS)\n",
        "\n",
        "        # TODO: Cleaner program (merge copy-paste) !!\n",
        "\n",
        "    def loadLines(self, fileName, fields):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fileName (str): file to load\n",
        "            field (set<str>): fields to extract\n",
        "        Return:\n",
        "            dict<dict<str>>: the extracted fields for each line\n",
        "        \"\"\"\n",
        "        lines = {}\n",
        "\n",
        "        with open(fileName, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
        "            for line in f:\n",
        "                values = line.split(\" +++$+++ \")\n",
        "\n",
        "                # Extract fields\n",
        "                lineObj = {}\n",
        "                for i, field in enumerate(fields):\n",
        "                    lineObj[field] = values[i]\n",
        "\n",
        "                lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "        return lines\n",
        "\n",
        "    def loadConversations(self, fileName, fields):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fileName (str): file to load\n",
        "            field (set<str>): fields to extract\n",
        "        Return:\n",
        "            list<dict<str>>: the extracted fields for each line\n",
        "        \"\"\"\n",
        "        conversations = []\n",
        "\n",
        "        with open(fileName, 'r', encoding='iso-8859-1') as f:  # TODO: Solve Iso encoding pb !\n",
        "            for line in f:\n",
        "                values = line.split(\" +++$+++ \")\n",
        "\n",
        "                # Extract fields\n",
        "                convObj = {}\n",
        "                for i, field in enumerate(fields):\n",
        "                    convObj[field] = values[i]\n",
        "\n",
        "                # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
        "                lineIds = ast.literal_eval(convObj[\"utteranceIDs\"])\n",
        "\n",
        "                # Reassemble lines\n",
        "                convObj[\"lines\"] = []\n",
        "                for lineId in lineIds:\n",
        "                    convObj[\"lines\"].append(self.lines[lineId])\n",
        "\n",
        "                conversations.append(convObj)\n",
        "\n",
        "        return conversations\n",
        "\n",
        "    def getConversations(self):\n",
        "        return self.conversations\n",
        "\n",
        "\n",
        "# Based on code from https://github.com/AlJohri/OpenSubtitles\n",
        "# by Al Johri <al.johri@gmail.com>\n",
        "\n",
        "\"\"\"\n",
        "Load the opensubtitles dialog corpus.\n",
        "\"\"\"\n",
        "\n",
        "class OpensubsData:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dirName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dirName (string): directory where to load the corpus\n",
        "        \"\"\"\n",
        "\n",
        "        # Hack this to filter on subset of Opensubtitles\n",
        "        # dirName = \"%s/en/Action\" % dirName\n",
        "\n",
        "        print(\"Loading OpenSubtitles conversations in %s.\" % dirName)\n",
        "        self.conversations = []\n",
        "        self.tag_re = re.compile(r'(<!--.*?-->|<[^>]*>)')\n",
        "        self.conversations = self.loadConversations(dirName)\n",
        "\n",
        "    def loadConversations(self, dirName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dirName (str): folder to load\n",
        "        Return:\n",
        "            array(question, answer): the extracted QA pairs\n",
        "        \"\"\"\n",
        "        conversations = []\n",
        "        dirList = self.filesInDir(dirName)\n",
        "        for filepath in tqdm(dirList, \"OpenSubtitles data files\"):\n",
        "            if filepath.endswith('xml'):\n",
        "                try:\n",
        "                    doc = self.getXML(filepath)\n",
        "                    conversations.extend(self.genList(doc))\n",
        "                except ValueError:\n",
        "                    tqdm.write(\"Skipping file %s with errors.\" % filepath)\n",
        "                except:\n",
        "                    print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "                    raise\n",
        "        return conversations\n",
        "\n",
        "    def getConversations(self):\n",
        "        return self.conversations\n",
        "\n",
        "    def genList(self, tree):\n",
        "        root = tree.getroot()\n",
        "\n",
        "        timeFormat = '%H:%M:%S'\n",
        "        maxDelta = datetime.timedelta(seconds=1)\n",
        "\n",
        "        startTime = datetime.datetime.min\n",
        "        strbuf = ''\n",
        "        sentList = []\n",
        "\n",
        "        for child in root:\n",
        "            for elem in child:\n",
        "                if elem.tag == 'time':\n",
        "                    elemID = elem.attrib['id']\n",
        "                    elemVal = elem.attrib['value'][:-4]\n",
        "                    if elemID[-1] == 'S':\n",
        "                        startTime = datetime.datetime.strptime(elemVal, timeFormat)\n",
        "                    else:\n",
        "                        sentList.append((strbuf.strip(), startTime, datetime.datetime.strptime(elemVal, timeFormat)))\n",
        "                        strbuf = ''\n",
        "                else:\n",
        "                    try:\n",
        "                        strbuf = strbuf + \" \" + elem.text\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        conversations = []\n",
        "        for idx in range(0, len(sentList) - 1):\n",
        "            cur = sentList[idx]\n",
        "            nxt = sentList[idx + 1]\n",
        "            if nxt[1] - cur[2] <= maxDelta and cur and nxt:\n",
        "                tmp = {}\n",
        "                tmp[\"lines\"] = []\n",
        "                tmp[\"lines\"].append(self.getLine(cur[0]))\n",
        "                tmp[\"lines\"].append(self.getLine(nxt[0]))\n",
        "                if self.filter(tmp):\n",
        "                    conversations.append(tmp)\n",
        "\n",
        "        return conversations\n",
        "\n",
        "    def getLine(self, sentence):\n",
        "        line = {}\n",
        "\n",
        "        temptext = self.tag_re.sub('', sentence).replace('\\\\\\'','\\'').strip().lower()\n",
        "\n",
        "        if temptext.startswith(\"-\"):\n",
        "            line[\"text\"] = temptext[2:]\n",
        "        else:\n",
        "            line[\"text\"] = temptext\n",
        "            \n",
        "        return line\n",
        "\n",
        "    def filter(self, lines):\n",
        "        # Use the followint to customize filtering of QA pairs\n",
        "        #\n",
        "        # startwords = (\"what\", \"how\", \"when\", \"why\", \"where\", \"do\", \"did\", \"is\", \"are\", \"can\", \"could\", \"would\", \"will\")\n",
        "        # question = lines[\"lines\"][0][\"text\"]\n",
        "        # if not question.endswith('?'):\n",
        "        #     return False\n",
        "        # if not question.split(' ')[0] in startwords:\n",
        "        #     return False\n",
        "        #\n",
        "        return True\n",
        "\n",
        "    def getXML(self, filepath):\n",
        "        fext = os.path.splitext(filepath)[1]\n",
        "        if fext == '.gz':\n",
        "            tmp = GzipFile(filename=filepath)\n",
        "            return ET.parse(tmp) #, parser=ET.XMLParser(encoding=\"utf8\"))\n",
        "        else:\n",
        "            return ET.parse(filepath) #, parser=ET.XMLParser(encoding=\"utf8\"))\n",
        "\n",
        "    def filesInDir(self, dirname):\n",
        "        result = []\n",
        "        for dirpath, dirs, files in os.walk(dirname):\n",
        "            for filename in files:\n",
        "                fname = os.path.join(dirpath, filename)\n",
        "                result.append(fname)\n",
        "        return result\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Load transcripts from the Supreme Court of the USA.\n",
        "Available from here:\n",
        "https://github.com/pender/chatbot-rnn\n",
        "\"\"\"\n",
        "\n",
        "class ScotusData:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dirName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dirName (string): directory where to load the corpus\n",
        "        \"\"\"\n",
        "        self.lines = self.loadLines(os.path.join(dirName, \"scotus\"))\n",
        "        self.conversations = [{\"lines\": self.lines}]\n",
        "\n",
        "\n",
        "    def loadLines(self, fileName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fileName (str): file to load\n",
        "        Return:\n",
        "            list<dict<str>>: the extracted fields for each line\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "\n",
        "        with open(fileName, 'r') as f:\n",
        "            for line in f:\n",
        "                l = line[line.index(\":\")+1:].strip()  # Strip name of speaker.\n",
        "\n",
        "                lines.append({\"text\": l})\n",
        "\n",
        "        return lines\n",
        "\n",
        "\n",
        "    def getConversations(self):\n",
        "        return self.conversations\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Ubuntu Dialogue Corpus\n",
        "http://arxiv.org/abs/1506.08909\n",
        "\"\"\"\n",
        "\n",
        "class UbuntuData:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dirName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dirName (string): directory where to load the corpus\n",
        "        \"\"\"\n",
        "        self.MAX_NUMBER_SUBDIR = np.inf #10\n",
        "        self.conversations = []\n",
        "        __dir = os.path.join(dirName, \"dialogs\")\n",
        "        number_subdir = 0\n",
        "        for sub in tqdm(os.scandir(__dir), desc=\"Ubuntu dialogs subfolders\", total=len(os.listdir(__dir))):\n",
        "            if number_subdir == self.MAX_NUMBER_SUBDIR:\n",
        "                print(\"WARNING: Early stoping, only extracting {} directories\".format(self.MAX_NUMBER_SUBDIR))\n",
        "                return\n",
        "\n",
        "            if sub.is_dir():\n",
        "                number_subdir += 1\n",
        "                for f in os.scandir(sub.path):\n",
        "                    if f.name.endswith(\".tsv\"):\n",
        "                        try:\n",
        "                            self.conversations.append({\"lines\": self.loadLines(f.path)})\n",
        "                        except ValueError:\n",
        "                            tqdm.write(\"Skipping file %s with errors.\" % f.path)\n",
        "                        except:\n",
        "                            print(\"Unexpected error:\", sys.exc_info()[0])\n",
        "                            raise\n",
        "\n",
        "    def loadLines(self, fileName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fileName (str): file to load\n",
        "        Return:\n",
        "            list<dict<str>>: the extracted fields for each line\n",
        "        \"\"\"\n",
        "        lines = []\n",
        "        with open(fileName, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                l = line[line.rindex(\"\\t\")+1:].strip()  # Strip metadata (timestamps, speaker names)\n",
        "\n",
        "                lines.append({\"text\": l})\n",
        "\n",
        "        return lines\n",
        "\n",
        "\n",
        "    def getConversations(self):\n",
        "        return self.conversations\n",
        "\n",
        "\"\"\"\n",
        "Load data from a dataset of simply-formatted data\n",
        "from A to B\n",
        "from B to A\n",
        "from A to B\n",
        "from B to A\n",
        "from A to B\n",
        "===\n",
        "from C to D\n",
        "from D to C\n",
        "from C to D\n",
        "from D to C\n",
        "from C to D\n",
        "from D to C\n",
        "...\n",
        "`===` lines just separate linear conversations between 2 people.\n",
        "\"\"\"\n",
        "\n",
        "class LightweightData:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lightweightFile):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lightweightFile (string): file containing our lightweight-formatted corpus\n",
        "        \"\"\"\n",
        "        self.CONVERSATION_SEP = \"===\"\n",
        "        self.conversations = []\n",
        "        self.loadLines(lightweightFile + '.txt')\n",
        "\n",
        "    def loadLines(self, fileName):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fileName (str): file to load\n",
        "        \"\"\"\n",
        "\n",
        "        linesBuffer = []\n",
        "        with open(fileName, 'r') as f:\n",
        "            for line in f:\n",
        "                l = line.strip()\n",
        "                if l == self.CONVERSATION_SEP:\n",
        "                    self.conversations.append({\"lines\": linesBuffer})\n",
        "                    linesBuffer = []\n",
        "                else:\n",
        "                    linesBuffer.append({\"text\": l})\n",
        "            if len(linesBuffer):  # Eventually flush the last conversation\n",
        "                self.conversations.append({\"lines\": linesBuffer})\n",
        "\n",
        "    def getConversations(self):\n",
        "        return self.conversations\n",
        "\n",
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "\n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;#+_]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z ]')\n",
        "    #stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    #text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "    text = ' '.join([x for x in text.split() if x])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def extractText(line, fast_preprocessing=True):\n",
        "    if fast_preprocessing:\n",
        "\n",
        "        \"\"\"\n",
        "        GOOD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
        "        REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;#+_]')\n",
        "        REPLACE_SEVERAL_SPACES = re.compile('\\s+')\n",
        "\n",
        "        line = line.lower()\n",
        "        line = REPLACE_BY_SPACE_RE.sub(' ', line)\n",
        "        line = GOOD_SYMBOLS_RE.sub('', line)\n",
        "        line = REPLACE_SEVERAL_SPACES.sub(' ', line)\n",
        "        return line.strip()\n",
        "        \"\"\"\n",
        "\n",
        "        return text_prepare(line)\n",
        "\n",
        "    else:\n",
        "        return nltk.word_tokenize(line.lower())\n",
        "\n",
        "\n",
        "def splitConversations(conversations, max_len=20, fast_preprocessing=True):\n",
        "    data = []\n",
        "    for i, conversation in enumerate(tqdm(conversations)):\n",
        "        lines = conversation['lines']\n",
        "        for i in range(len(lines) - 1):\n",
        "            request = extractText(lines[i]['text'], fast_preprocessing=fast_preprocessing)\n",
        "            reply = extractText(lines[i + 1]['text'], fast_preprocessing=fast_preprocessing)\n",
        "            if 0 < len(request) <= max_len and 0 < len(reply) <= max_len:\n",
        "                data += [(request, reply)]\n",
        "    return data\n",
        "\n",
        "\n",
        "def readCornellData(path, max_len=20, fast_preprocessing=True):\n",
        "    dataset = CornellData(path)\n",
        "    conversations = dataset.getConversations()\n",
        "    return splitConversations(conversations, max_len=max_len, fast_preprocessing=fast_preprocessing)\n",
        "\n",
        "\n",
        "def readOpensubsData(path, max_len=20, fast_preprocessing=True):\n",
        "    dataset = OpensubsData(path)\n",
        "    conversations = dataset.getConversations()\n",
        "    return splitConversations(conversations, max_len=max_len, fast_preprocessing=fast_preprocessing)\n",
        "\n",
        "def readScotusData(path, max_len=20, fast_preprocessing=True):\n",
        "    dataset = ScotusData(path)\n",
        "    conversations = dataset.getConversations()\n",
        "    return splitConversations(conversations, max_len=max_len, fast_preprocessing=fast_preprocessing)\n",
        "\n",
        "def readUbuntuData(path, max_len=20, fast_preprocessing=True):\n",
        "    dataset = UbuntuData(path)\n",
        "    conversations = dataset.getConversations()\n",
        "    return splitConversations(conversations, max_len=max_len, fast_preprocessing=fast_preprocessing)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdQVvjMZXH-k",
        "colab_type": "code",
        "outputId": "19b4a1b0-3a01-46ae-fa6f-1d9ce72862e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/HonorProject/NLP Honor\"\n",
        "\n",
        "choices=[\"cornell\"]#, \"scotus\", \"opensubs\"] #, \"ubuntu\"]\n",
        "Classes = [CornellData]#, ScotusData, OpensubsData] #, UbuntuData]\n",
        "\n",
        "\n",
        "for choice, Class in zip(choices, Classes):\n",
        "    print(choice, Class, \"started\")\n",
        "    dataset_path = os.path.join(\"data\", choice)\n",
        "    dataset = Class(dataset_path)\n",
        "    conversations = dataset.getConversations()\n",
        "\n",
        "    print(len(conversations))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/HonorProject/NLP Honor\n",
            "cornell <class '__main__.CornellData'> started\n",
            "83097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osRIMhZvX9Fd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Data Preprocessing\n",
        "1. **NLTK tokenizer is used for initial preprocessing lines from the dataset. But punctuations like \",\",  \".\", \"?\" etc. are weighted as same as words to capture the tone and structure of the sentences.**\n",
        "\n",
        "2. For each preprocessed sentence, the original sentence is also stored for later.\n",
        "\n",
        "3. Here, only the cornell dataset is shown. The other ones were considered in separate experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY7lxkLiXiuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "82a973f1-74c4-453e-ad5e-3e1fca00a0e5"
      },
      "source": [
        "data = []\n",
        "for i, conversation in enumerate(tqdm(conversations)):\n",
        "    lines = conversation['lines']\n",
        "    for i in range(len(lines)):\n",
        "        line = extractText(lines[i]['text'].lower(), fast_preprocessing=False)\n",
        "        \n",
        "        line = ' '.join(line)\n",
        "\n",
        "        data += [(line, lines[i]['text'])]\n",
        "\n",
        "print(len(data))\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [00:51<00:00, 1607.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "304713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIr3rj-EYiRk",
        "colab_type": "text"
      },
      "source": [
        "**The individual lines are grouped to form conversations. In this case, conversation lines are considered bidirectional. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8bYwYWfdCuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def splitConversationsHere(conversations):\n",
        "    data = []\n",
        "    for i, conversation in enumerate(tqdm(conversations)):\n",
        "        lines = conversation['lines']\n",
        "        for i in range(len(lines) - 1):\n",
        "            request = extractText(lines[i]['text'].lower(), fast_preprocessing=False)\n",
        "            request = ' '.join(request)\n",
        "\n",
        "            reply = extractText(lines[i + 1]['text'].lower(), fast_preprocessing=False)\n",
        "            reply = ' '.join(reply)\n",
        "            \n",
        "            data += [(request, reply, lines[i]['text'], lines[i + 1]['text'])]\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mY1udGw4ice",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4aa58b06-43de-40ce-f56b-93dd1d63cc8d"
      },
      "source": [
        "convs = splitConversationsHere(conversations)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 83097/83097 [01:13<00:00, 1131.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSUOMBs8fQLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dataDF = pd.DataFrame(data=data)\n",
        "convDF = pd.DataFrame(data=convs)\n",
        "alllines = pd.DataFrame(data=dataDF[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udBobhUdgbmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataDF.to_csv(path_or_buf=\"all_lines_DF_only_cornell.txt\", index=False, header=False)\n",
        "convDF.to_csv(path_or_buf=\"all_convs_DF_only_cornell.txt\", index=False, header=False)\n",
        "alllines.to_csv(path_or_buf=\"all_lines_txt_only_cornell.txt\", index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jXC7d8qZAYU",
        "colab_type": "text"
      },
      "source": [
        "To learn sentence embeddings, **sent2vec** module was used. The module can be found there:\n",
        "\n",
        "[sent2vec](https://github.com/epfml/sent2vec)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affiTWCGSq_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"!wget -O sent2vec.zip https://codeload.github.com/epfml/sent2vec/zip/master\n",
        "!unzip sent2vec.zip \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwpTFTswUzlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \"/content/drive/My Drive/sent2vec/sent2vec-master\"\n",
        "!pip install .\n",
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDRRDjfxWWLc",
        "colab_type": "code",
        "outputId": "75e4b01d-2a78-4d0f-e9ad-13576117271d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/sent2vec/\""
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/sent2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q-p6neSZcE-",
        "colab_type": "text"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "\n",
        "**Unsupervised embeddings for the whole sentences were learnt on the corpus data. The training was run for 9 epochs to learn embeddings of dimention 300.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkrFCE0-q_4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! sent2vec-master/./fasttext sent2vec -input \"/content/drive/My Drive/HonorProject/NLP Honor/all_lines_txt_only_cornell.txt\" -output my_sent2vec_cornell_model2 -dropoutK 0 -dim 300 -epoch 9 -lr 0.2 -thread 10 -bucket 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K23S2Vkzr43V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "967601b1-4da7-40a5-bf30-68ce21326660"
      },
      "source": [
        "%cd \"/content/drive/My Drive/sent2vec/\"\n",
        "import sent2vec\n",
        "sent2vec_model = sent2vec.Sent2vecModel()\n",
        "sent2vec_model.load_model('my_sent2vec_cornell_model.bin')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/sent2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFFjWDpKr1Rm",
        "colab_type": "text"
      },
      "source": [
        "# Experimental Observation\n",
        "\n",
        "As embeddings are learnt for the complete sentence as a whole, it works better than word embedding based approaches, as the order of the words are also taken into consideration here. The puctuations and the tone in the sentence is also taken into account, as confirmed by the difference in responses depending on the tone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1q72hPBsdjy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4167f4e8-d070-4887-e1fe-9a12b89aa614"
      },
      "source": [
        "predict(\"I don't know.\")"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well, I recommend knowing before speaking. The law leaves much room for interpretation -- but very little for self-doubt.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sq8q5NWs5Y2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0c3ab52-338d-491e-fb26-409694a5ef77"
      },
      "source": [
        "predict(\"I don't know?\")"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You do, you do. You're just not saying.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qFGuFpjZ3EN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##Caching\n",
        "\n",
        "**Using the learnt model, sentence embeddings were calculated and stored for all the lines in the dataset. This would require less computation during inference. This would also help in better RAM memory utilization when the bot will be inactive.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8K-DgKOuPPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "#os.makedirs(\"Sentence_Embeddings\", exist_ok=True)\n",
        "import numpy as np\n",
        "\n",
        "count = alllines.shape[0]\n",
        "sent_vectors = np.zeros((count, 300), dtype=np.float32)\n",
        "\n",
        "sent_ids = alllines[0].values\n",
        "\n",
        "for i, row in alllines.iterrows():\n",
        "\n",
        "    sentence = row[0]\n",
        "    emb_sent2vec = sent2vec_model.embed_sentence(sentence)\n",
        "    sent_vectors[i, :] = emb_sent2vec[0]\n",
        "    \n",
        "with open(\"Sentence_Embeddings/sent2vec_vectors_only_cornell.pickle\", 'wb') as f:\n",
        "    # Pickle the 'data' dictionary using the highest protocol available.\n",
        "    pickle.dump((sent_ids, sent_vectors), f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z3EruopbD3Z",
        "colab_type": "text"
      },
      "source": [
        "# Inference\n",
        "\n",
        "1. When a new sentence comes along, the embedding for the sentence as a whole is extracted from the model. \n",
        "\n",
        "2. Then nearest neighboring is used to find the closest sentence in the corpus. \n",
        "\n",
        "3. Based on the nearest neighbor, the sentence from the conversation involving that neighbor is returned. If there are multiple choices, then one at random is returned for element of surprise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S214DSyye0CZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b565eabd-56b2-4471-e213-4b68f9d020ca"
      },
      "source": [
        "%cd \"/content/drive/My Drive/HonorProject/NLP Honor\"\n",
        "import pandas as pd\n",
        "\n",
        "convDF = pd.read_csv(\"all_convs_DF_only_cornell.txt\", header=None)\n",
        "alllines = pd.read_csv(\"all_lines_txt_only_cornell.txt\", header=None)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/HonorProject/NLP Honor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I42JRVdA1xe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import pairwise_distances_argmin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9fM6vkdlvBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "sent_ids, sent_vectors = pickle.load(open('sent2vec_vectors_only_cornell.pickle','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARUYy6Rg2hWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(st):\n",
        "\n",
        "    st = nltk.word_tokenize(st.lower())\n",
        "    st = ' '.join(st)\n",
        "    arg = pairwise_distances_argmin(sent2vec_model.embed_sentence(st),sent_vectors, metric='cosine')[0]\n",
        "\n",
        "    which = sent_ids[arg]\n",
        "\n",
        "    try:\n",
        "        response = random.choice(convDF.loc[convDF[0] == which][3].values).strip()\n",
        "    except:\n",
        "      \tresponse = random.choice(convDF.loc[convDF[1] == which][2].values).strip()\n",
        "\n",
        "    return response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuZYYjGjijYc",
        "colab_type": "text"
      },
      "source": [
        "**The following are some examples of responses:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGw8XiH7BLpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b7a511a-3279-4714-e765-d41022ff2afc"
      },
      "source": [
        "predict(\"What are you doing?\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You know what Mr. White said. A photographer always goes after a story.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgQxG7XLjnld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "618f5677-8a1b-461a-e0ad-50682cdab08f"
      },
      "source": [
        "predict(\"Is it raining?\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yeah, just started.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT3z3pUwi5Ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c33e9f50-a725-43a0-ea9a-e96a0294b3d9"
      },
      "source": [
        "predict(\"What is your name?\")"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's cool.  We're gettin' to know each other. This is a good thing. I'm Carter.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3yIvRgfoIwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03d90fab-9680-4948-fb8b-16e7635fc650"
      },
      "source": [
        "predict(\"Go away.\")"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You're supposed to watch me and entertain me, and make me appreciate the brief but happy years of childhood.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrVbOtl9ofwa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPgy1K4qAgZr",
        "colab_type": "text"
      },
      "source": [
        "# Of course, there are some bad and weird responses too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX_g9CQ4oghL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24954042-5d71-49bc-d6ba-aeb8d33b936e"
      },
      "source": [
        "predict(\"I am sad.\")"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I assume you are loitering here to learn what efficiency rating I plan to give your cadets.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLvvKni9oseW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b08c203-19e2-40e7-fca3-3dcabd961362"
      },
      "source": [
        "predict(\"what is your aim?\")"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm a drunkard.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T6mD0kbo1gx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dfbe392-e5f5-4d94-8445-9cc37a827e8b"
      },
      "source": [
        "predict(\"What is your age?\")"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'To find the Holy Grail.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubWIDYFVpBXJ",
        "colab_type": "text"
      },
      "source": [
        "# Probable Problems\n",
        "\n",
        "\n",
        "\n",
        "1.   Training data was not diverse\n",
        "2.   Trained for a short number of epochs\n",
        "3. Need to do selective modeling. Probably a better inference method could be learnt.\n",
        "4. casual responses, can not carry out meaningful conversations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribth4xiqUSR",
        "colab_type": "text"
      },
      "source": [
        "# Alternative\n",
        "\n",
        "**I would encourage you too look at the other approch (provided link to the notebook in the beginning) and provide suggestions if you like! Although still experimental, that approach is more appropriate. Afterall, conversations are rarely selective, are they?!**\n",
        "\n",
        "[Providing the link here again!](https://github.com/ragibhassantonoy/NLP_Honor/blob/master/attention_week5_honor.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18uiUL48o9NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}