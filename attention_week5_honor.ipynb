{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of Copy of week5-honor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwsIEFRWwEc1",
        "colab_type": "code",
        "outputId": "c234e94e-2412-41e7-9232-d24b41c2dd90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()  \n",
        "# setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_project()\n",
        "setup_google_colab.setup_honor()\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\\nimport setup_google_colab\\n# please, uncomment the week you're working on\\n# setup_google_colab.setup_week1()  \\n# setup_google_colab.setup_week2()\\n# setup_google_colab.setup_week3()\\n# setup_google_colab.setup_week4()\\n# setup_google_colab.setup_project()\\nsetup_google_colab.setup_honor()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LojRpLN15-qr",
        "colab_type": "code",
        "outputId": "1f03589a-9287-45f2-d1b1-51716f239034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/HonorProject/NLP Honor\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/HonorProject/NLP Honor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnPLgeGa2ih3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## select max_len of 100 or 200 #####################################\n",
        "########## from cornell, opensubs, scotus ############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hjsm5RH7DO9",
        "colab_type": "code",
        "outputId": "7014de44-5946-411a-abf7-a1bd11f65ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, LayerNormalization\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda, Reshape, Embedding, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model, Model, Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\"\"\"import keras\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply #, LayerNormalization\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda, Reshape, Embedding, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model, Model, Sequential\n",
        "import keras.backend as K\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tqdm import tqdm\n",
        "#from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mg0vmgpN2DX",
        "colab_type": "code",
        "outputId": "8d60a3cd-c377-4bec-d692-4cf1e9a66d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.2.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: six, protobuf, h5py, wrapt, gast, google-pasta, tensorflow-estimator, scipy, absl-py, keras-preprocessing, opt-einsum, grpcio, wheel, astunparse, termcolor, tensorboard, numpy\n",
            "Required-by: fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkARSce74Azu",
        "colab_type": "code",
        "outputId": "1ee1d327-ac2a-4a63-c3bb-ef29346dc234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from datasets import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cDXpvV4eaL",
        "colab_type": "code",
        "outputId": "86f6bb58-d5ba-485e-f1f7-3799c6516fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"choices=[\"cornell\", \"scotus\", \"opensubs\"] #, \"ubuntu\"]\n",
        "Classes = [CornellData, ScotusData, OpensubsData] #, UbuntuData]\n",
        "\n",
        "for choice, Class in zip(choices, Classes):\n",
        "    print(choice, Class, \"started\")\n",
        "    dataset_path = os.path.join(\"data\", choice)\n",
        "    dataset = Class(dataset_path)\n",
        "    conversations = dataset.getConversations()\n",
        "\n",
        "    with open(\"conversations/d_\" + str(choice) + '_convs.pickle', 'wb') as f:\n",
        "        # Pickle the 'data' dictionary using the highest protocol available.\n",
        "        pickle.dump(conversations, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    print(choice, Class, \"dumped\")\n",
        "    print()\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'choices=[\"cornell\", \"scotus\", \"opensubs\"] #, \"ubuntu\"]\\nClasses = [CornellData, ScotusData, OpensubsData] #, UbuntuData]\\n\\nfor choice, Class in zip(choices, Classes):\\n    print(choice, Class, \"started\")\\n    dataset_path = os.path.join(\"data\", choice)\\n    dataset = Class(dataset_path)\\n    conversations = dataset.getConversations()\\n\\n    with open(\"conversations/d_\" + str(choice) + \\'_convs.pickle\\', \\'wb\\') as f:\\n        # Pickle the \\'data\\' dictionary using the highest protocol available.\\n        pickle.dump(conversations, f, pickle.HIGHEST_PROTOCOL)\\n\\n    print(choice, Class, \"dumped\")\\n    print()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lQdYcDl7V0c",
        "colab_type": "code",
        "outputId": "3fa3c15d-e6fc-4cf9-b6bf-030633b8647b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"choices=[\"cornell\", \"scotus\", \"opensubs\"]#, \"ubuntu\"]\n",
        "\n",
        "conversations={}\n",
        "for choice in choices:\n",
        "    print(choice, \"size: \")\n",
        "\n",
        "    with open(\"conversations/d_\" + str(choice) + '_convs.pickle', 'rb') as f:\n",
        "        # The protocol version used is detected automatically, so we do not\n",
        "        # have to specify it.\n",
        "        conversations[choice] = pickle.load(f)\n",
        "\n",
        "    print(len(conversations[choice]))\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'choices=[\"cornell\", \"scotus\", \"opensubs\"]#, \"ubuntu\"]\\n\\nconversations={}\\nfor choice in choices:\\n    print(choice, \"size: \")\\n\\n    with open(\"conversations/d_\" + str(choice) + \\'_convs.pickle\\', \\'rb\\') as f:\\n        # The protocol version used is detected automatically, so we do not\\n        # have to specify it.\\n        conversations[choice] = pickle.load(f)\\n\\n    print(len(conversations[choice]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4glN18PF3j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_word_len=25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDVpjj5DzGeJ",
        "colab_type": "code",
        "outputId": "5c6013d1-28f4-4c57-cd5c-4532b57f5409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"choices=[\"cornell\", \"scotus\", \"opensubs\"]#, \"ubuntu\"]\n",
        "\n",
        "datas = []\n",
        "for choice in choices:\n",
        "    data = splitConversations(conversations[choice], max_len, fast_preprocessing=True)\n",
        "    datas.extend(data)\n",
        "\n",
        "print(\"total convs:\", len(datas))\n",
        "\n",
        "with open(\"conversations/dataset\"+str(max_len)+'chars_without_ubuntu.pickle', 'wb') as f:\n",
        "        # Pickle the 'data' dictionary using the highest protocol available.\n",
        "        pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'choices=[\"cornell\", \"scotus\", \"opensubs\"]#, \"ubuntu\"]\\n\\ndatas = []\\nfor choice in choices:\\n    data = splitConversations(conversations[choice], max_len, fast_preprocessing=True)\\n    datas.extend(data)\\n\\nprint(\"total convs:\", len(datas))\\n\\nwith open(\"conversations/dataset\"+str(max_len)+\\'chars_without_ubuntu.pickle\\', \\'wb\\') as f:\\n        # Pickle the \\'data\\' dictionary using the highest protocol available.\\n        pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsM9g88k8pnk",
        "colab_type": "code",
        "outputId": "b46cb6c5-214d-41a1-a8e6-72e1bd01d33e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"with open(\"conversations/dataset\"+str(max_len)+'chars_without_ubuntu.pickle', 'rb') as f:\n",
        "    # The protocol version used is detected automatically, so we do not\n",
        "    # have to specify it.\n",
        "    datas = pickle.load(f)\n",
        "\n",
        "print(len(datas))\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'with open(\"conversations/dataset\"+str(max_len)+\\'chars_without_ubuntu.pickle\\', \\'rb\\') as f:\\n    # The protocol version used is detected automatically, so we do not\\n    # have to specify it.\\n    datas = pickle.load(f)\\n\\nprint(len(datas))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g-1PS6l8pk_",
        "colab_type": "code",
        "outputId": "0ffbb904-38f9-44bb-96f7-daf5e2c63c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"import pandas as pd\n",
        "columns = (\"request\", \"reply\", \"line1\", \"line2\")\n",
        "dataframe = pd.DataFrame(data=datas, columns=columns)\n",
        "\n",
        "for i in range(5):\n",
        "    dataframe = dataframe.sample(frac=1)\n",
        "dataframe.to_csv(path_or_buf=\"conversations/datasetDF\"+str(max_len)+'chars_without_ubuntu.txt', index=False)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import pandas as pd\\ncolumns = (\"request\", \"reply\", \"line1\", \"line2\")\\ndataframe = pd.DataFrame(data=datas, columns=columns)\\n\\nfor i in range(5):\\n    dataframe = dataframe.sample(frac=1)\\ndataframe.to_csv(path_or_buf=\"conversations/datasetDF\"+str(max_len)+\\'chars_without_ubuntu.txt\\', index=False)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHLfAzs8phm",
        "colab_type": "code",
        "outputId": "a183099a-5e53-478c-ec7b-61b632f0f571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"dataframe = pd.read_csv(\"conversations/datasetDF\"+str(max_len)+'chars_without_ubuntu.txt')\n",
        "print(dataframe)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataframe = pd.read_csv(\"conversations/datasetDF\"+str(max_len)+\\'chars_without_ubuntu.txt\\')\\nprint(dataframe)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-auFVjPRvmYy",
        "colab_type": "code",
        "outputId": "1a9f5e85-312b-4aef-dff7-13a18c8d7d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"max_word_len = 25\n",
        "choices=[\"cornell\"]#, \"scotus\", \"opensubs\"]#, \"ubuntu\"]\n",
        "\n",
        "conversations={}\n",
        "for choice in choices:\n",
        "    print(choice, \"size: \")\n",
        "\n",
        "    with open(\"conversations/d_\" + str(choice) + '_convs.pickle', 'rb') as f:\n",
        "        # The protocol version used is detected automatically, so we do not\n",
        "        # have to specify it.\n",
        "        conversations[choice] = pickle.load(f)\n",
        "\n",
        "    print(len(conversations[choice]))\n",
        "\n",
        "datas = []\n",
        "for choice in choices:\n",
        "    data = splitConversations(conversations[choice], max_word_len, fast_preprocessing=False)\n",
        "    datas.extend(data)\n",
        "\n",
        "print(\"total convs:\", len(datas))\n",
        "\n",
        "\n",
        "print(datas[0])\n",
        "\n",
        "with open(\"conversations/dataset_nltk_\"+str(max_len)+'words_only_cornell.pickle', 'wb') as f:\n",
        "        # Pickle the 'data' dictionary using the highest protocol available.\n",
        "        pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "import pandas as pd\n",
        "columns = (\"request\", \"reply\", \"line1\", \"line2\")\n",
        "dataframe = pd.DataFrame(data=datas, columns=columns)\n",
        "\n",
        "for i in range(5):\n",
        "    dataframe = dataframe.sample(frac=1)\n",
        "dataframe.to_csv(path_or_buf=\"conversations/dataset_nltk_DF\"+str(max_len)+'words_only_cornell.txt', index=False)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'max_word_len = 25\\nchoices=[\"cornell\"]#, \"scotus\", \"opensubs\"]#, \"ubuntu\"]\\n\\nconversations={}\\nfor choice in choices:\\n    print(choice, \"size: \")\\n\\n    with open(\"conversations/d_\" + str(choice) + \\'_convs.pickle\\', \\'rb\\') as f:\\n        # The protocol version used is detected automatically, so we do not\\n        # have to specify it.\\n        conversations[choice] = pickle.load(f)\\n\\n    print(len(conversations[choice]))\\n\\ndatas = []\\nfor choice in choices:\\n    data = splitConversations(conversations[choice], max_word_len, fast_preprocessing=False)\\n    datas.extend(data)\\n\\nprint(\"total convs:\", len(datas))\\n\\n\\nprint(datas[0])\\n\\nwith open(\"conversations/dataset_nltk_\"+str(max_len)+\\'words_only_cornell.pickle\\', \\'wb\\') as f:\\n        # Pickle the \\'data\\' dictionary using the highest protocol available.\\n        pickle.dump(datas, f, pickle.HIGHEST_PROTOCOL)\\n\\nimport pandas as pd\\ncolumns = (\"request\", \"reply\", \"line1\", \"line2\")\\ndataframe = pd.DataFrame(data=datas, columns=columns)\\n\\nfor i in range(5):\\n    dataframe = dataframe.sample(frac=1)\\ndataframe.to_csv(path_or_buf=\"conversations/dataset_nltk_DF\"+str(max_len)+\\'words_only_cornell.txt\\', index=False)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9FXNM2fyltc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_word_len = 25\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOe_XVu181u6",
        "colab_type": "code",
        "outputId": "e3d68ca9-de70-4487-b94a-82298c380b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_word_len = 25\n",
        "\n",
        "with open(\"conversations/dataset_nltk_\"+str(max_word_len)+'words_only_cornell.pickle', 'rb') as f:\n",
        "    # The protocol version used is detected automatically, so we do not\n",
        "    # have to specify it.\n",
        "    datas = pickle.load(f)\n",
        "\n",
        "print(len(datas))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "169273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOJzebUL7miF",
        "colab_type": "code",
        "outputId": "bec71733-1eec-45e4-99b1-be622492b823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "' '.join(datas[0][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i do n't fucking know . what am i , his biographer ? olaf , what part of russia are you from ?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HivJvrvE8pQj",
        "colab_type": "code",
        "outputId": "f469d95e-66a8-4c05-b6da-9781eb866c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"tok_word = text.Tokenizer(lower=True, oov_token=\"<OOV>\")\n",
        "tok_word.fit_on_texts(list(np.array(datas)[:,0])+list(np.array(datas)[:,1]))\n",
        "word_index = tok_word.word_index\n",
        "print(len(word_index))\n",
        "inv_word_index = {v:k for k,v in word_index.items()}\n",
        "\n",
        "START_TOKEN, END_TOKEN = len(word_index)+1, len(word_index)+2\n",
        "\n",
        "# Vocabulary size plus start and end token\n",
        "VOCAB_SIZE = len(word_index) + 3\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tok_word = text.Tokenizer(lower=True, oov_token=\"<OOV>\")\\ntok_word.fit_on_texts(list(np.array(datas)[:,0])+list(np.array(datas)[:,1]))\\nword_index = tok_word.word_index\\nprint(len(word_index))\\ninv_word_index = {v:k for k,v in word_index.items()}\\n\\nSTART_TOKEN, END_TOKEN = len(word_index)+1, len(word_index)+2\\n\\n# Vocabulary size plus start and end token\\nVOCAB_SIZE = len(word_index) + 3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuBumsZE8pCV",
        "colab_type": "code",
        "outputId": "bf87b499-2291-40e9-a4b8-3737ab357e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"tok_pickle = {}\n",
        "\n",
        "tok_pickle[\"tok_word\"] = tok_word\n",
        "tok_pickle[\"word_index\"] = word_index\n",
        "tok_pickle[\"inv_word_index\"] = inv_word_index\n",
        "tok_pickle[\"START_TOKEN\"] = START_TOKEN\n",
        "tok_pickle[\"END_TOKEN\"] = END_TOKEN\n",
        "tok_pickle[\"VOCAB_SIZE\"] = VOCAB_SIZE\n",
        "\n",
        "with open(\"conversations/dataset_nltk_tokenizer_\"+str(max_word_len)+'words_only_cornell.pickle', 'wb') as f:\n",
        "        # Pickle the 'data' dictionary using the highest protocol available.\n",
        "        pickle.dump(tok_pickle, f, pickle.HIGHEST_PROTOCOL)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tok_pickle = {}\\n\\ntok_pickle[\"tok_word\"] = tok_word\\ntok_pickle[\"word_index\"] = word_index\\ntok_pickle[\"inv_word_index\"] = inv_word_index\\ntok_pickle[\"START_TOKEN\"] = START_TOKEN\\ntok_pickle[\"END_TOKEN\"] = END_TOKEN\\ntok_pickle[\"VOCAB_SIZE\"] = VOCAB_SIZE\\n\\nwith open(\"conversations/dataset_nltk_tokenizer_\"+str(max_word_len)+\\'words_only_cornell.pickle\\', \\'wb\\') as f:\\n        # Pickle the \\'data\\' dictionary using the highest protocol available.\\n        pickle.dump(tok_pickle, f, pickle.HIGHEST_PROTOCOL)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biKX16Lw-CaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"conversations/dataset_nltk_tokenizer_\"+str(max_word_len)+'words_only_cornell.pickle', 'rb') as f:\n",
        "    # The protocol version used is detected automatically, so we do not\n",
        "    # have to specify it.\n",
        "    tok_pickle = pickle.load(f)\n",
        "\n",
        "tok_word = tok_pickle[\"tok_word\"]\n",
        "word_index = tok_pickle[\"word_index\"] \n",
        "inv_word_index = tok_pickle[\"inv_word_index\"]\n",
        "START_TOKEN = tok_pickle[\"START_TOKEN\"]\n",
        "END_TOKEN = tok_pickle[\"END_TOKEN\"] \n",
        "VOCAB_SIZE = tok_pickle[\"VOCAB_SIZE\"] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_dUnvcgcuhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DFrequests = np.array(datas)[:,0]\n",
        "DFreplys = np.array(datas)[:,1]\n",
        "\n",
        "del datas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj69qr-7y51B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len = max_word_len+2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtOmRnExejPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_len = max_word_len+2\n",
        "DF_req_sequences = tok_word.texts_to_sequences(DFrequests)\n",
        "del DFrequests\n",
        "\n",
        "for i in range(len(DF_req_sequences)):\n",
        "    DF_req_sequences[i] = [START_TOKEN] + DF_req_sequences[i] + [END_TOKEN]    \n",
        "\n",
        "DF_req_pad_sequences = np.array(sequence.pad_sequences(DF_req_sequences, maxlen=max_seq_len, padding='post', truncating='post'))\n",
        "\n",
        "del DF_req_sequences\n",
        "\n",
        "DF_rep_sequences = tok_word.texts_to_sequences(DFreplys)\n",
        "del DFreplys\n",
        "\n",
        "for i in range(len(DF_rep_sequences)):\n",
        "    DF_rep_sequences[i] = [START_TOKEN] + DF_rep_sequences[i] + [END_TOKEN]    \n",
        "\n",
        "DF_rep_pad_sequences = np.array(sequence.pad_sequences(DF_rep_sequences, maxlen=max_seq_len, padding='post', truncating='post'))\n",
        "\n",
        "del DF_rep_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dxHB2PLLEMX",
        "colab_type": "code",
        "outputId": "0adfeb2d-5ab9-44e9-818a-28cee43018cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "n = 1500\n",
        "#print(DFreplys[n])\n",
        "print(DF_rep_pad_sequences[n])\n",
        "print(VOCAB_SIZE)\n",
        "l = [inv_word_index[i] for i in DF_rep_pad_sequences[n] if i!=0 and i<=len(inv_word_index)]\n",
        "' '.join(l)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[43124    55   391    33     8  1164     2     6    70  1515    77   161\n",
            "   197    16     2 43125     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "43126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"she knows not to steal . i 've taught her better than that .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc8z9iIggfF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### decode reply ##########################################\n",
        "\n",
        "def decode_answer(line, inv_vocab_index):\n",
        "    l = [inv_word_index[i] for i in line if i!=0  and i<=len(inv_word_index)]\n",
        "    msg = ' '.join(l)\n",
        "\n",
        "    return msg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJWGL0IpnlYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_one_hot_seqs(padded_sequences, vocab_size):\n",
        "    return np.array(list(map(lambda x: to_categorical(x, num_classes=vocab_size), padded_sequences)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGbRx8T2uZDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"X_temp, X_test, y_temp, y_test = train_test_split(DF_req_pad_sequences, DF_rep_pad_sequences, test_size=0.1, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1, random_state=0)\n",
        "\n",
        "del DF_req_pad_sequences\n",
        "del DF_rep_pad_sequences\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_val.shape, y_val.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "del X_test\n",
        "del y_test\n",
        "del X_val\n",
        "del y_val\"\"\"\n",
        "\n",
        "X_train = DF_req_pad_sequences\n",
        "y_train = DF_rep_pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PqijsUQhM6h",
        "colab_type": "code",
        "outputId": "c352778d-639a-4525-e678-10bb40ec3a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"Xoh_train = get_one_hot_seqs(X_train, word_index)\n",
        "Yoh_train = get_one_hot_seqs(y_train, word_index)\n",
        "\n",
        "Xoh_val = get_one_hot_seqs(X_val, word_index)\n",
        "Yoh_val = get_one_hot_seqs(y_val, word_index)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Xoh_train = get_one_hot_seqs(X_train, word_index)\\nYoh_train = get_one_hot_seqs(y_train, word_index)\\n\\nXoh_val = get_one_hot_seqs(X_val, word_index)\\nYoh_val = get_one_hot_seqs(y_val, word_index)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuxpttCHoHI_",
        "colab_type": "code",
        "outputId": "6db15d72-5184-411b-ab87-266cbf535158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"p = np.argmax(Yoh_train[1000], axis = -1)\n",
        "decode_answer(line = p, inv_vocab_index=inv_char_index, dataframe=dataframe)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'p = np.argmax(Yoh_train[1000], axis = -1)\\ndecode_answer(line = p, inv_vocab_index=inv_char_index, dataframe=dataframe)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWetBCIor6vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4acAkJer6rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-wvuR0rr6ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tx = max_seq_len\n",
        "Ty = max_seq_len-1\n",
        "\n",
        "n_a = 150 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "n_s = 300 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "embed_size = n_s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY9rYwepr6ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator1 = RepeatVector(Tx)\n",
        "concatenator1 = Concatenate(axis=-1)\n",
        "densor1 = Dense(100, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)\n",
        "concatenator2 = Concatenate(axis=-1)\n",
        "repeator2 = RepeatVector(1)\n",
        "repeator3 = RepeatVector(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05CzFsv2wjJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: one_step_attention\n",
        "\n",
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attention) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator1(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "    concat = concatenator1([a, s_prev])\n",
        "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "    energies = densor2(e)\n",
        "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(energies)\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k6nPRMgw-Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please note, this is the post attention LSTM cell.  \n",
        "# For the purposes of passing the automatic grader\n",
        "# please do not modify this global variable.  This will be corrected once the automatic grader is also updated.\n",
        "post_attention_LSTM_cell = LSTM(n_s, return_state = True, name='decoder') # post-attention LSTM \n",
        "post_attention_dense = Dense(embed_size) # post-attention LSTM\n",
        "#post_decoder_dense = Dense(n_s, activation='relu') \n",
        "output_layer = Dense(VOCAB_SIZE, activation=softmax)\n",
        "\n",
        "post_output_dense = Dense(embed_size) # post-attention LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utb4Ox8ACirK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget -O glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "#!sudo unzip glove.840B.300d.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0FY6msGCnVp",
        "colab_type": "code",
        "outputId": "2d13638c-780c-4e65-b3a7-c1396acc124a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"EMBEDDING_FILE = 'glove.840B.300d.txt'\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.rstrip().rsplit(' ')\n",
        "        word = values[0].lower()\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "n = 0\n",
        "#prepare embedding matrix\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    #if i >= max_features:\n",
        "    #    continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        n += 1\n",
        "\n",
        "print(n, \"not found.\")\n",
        "\n",
        "with open(\"conversations/glove300D_embedding_nltk_tokenizer_\"+str(max_word_len)+'words_only_cornell.pickle', 'wb') as f:\n",
        "        # Pickle the 'data' dictionary using the highest protocol available.\n",
        "        pickle.dump(embedding_matrix, f, pickle.HIGHEST_PROTOCOL)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EMBEDDING_FILE = \\'glove.840B.300d.txt\\'\\n\\nembeddings_index = {}\\nwith open(EMBEDDING_FILE,encoding=\\'utf8\\') as f:\\n    for line in f:\\n        values = line.rstrip().rsplit(\\' \\')\\n        word = values[0].lower()\\n        coefs = np.asarray(values[1:], dtype=\\'float32\\')\\n        embeddings_index[word] = coefs\\n\\nn = 0\\n#prepare embedding matrix\\nembedding_matrix = np.zeros((VOCAB_SIZE, embed_size))\\nfor word, i in word_index.items():\\n    #if i >= max_features:\\n    #    continue\\n    embedding_vector = embeddings_index.get(word)\\n    if embedding_vector is not None:\\n        # words not found in embedding index will be all-zeros.\\n        embedding_matrix[i] = embedding_vector\\n    else:\\n        n += 1\\n\\nprint(n, \"not found.\")\\n\\nwith open(\"conversations/glove300D_embedding_nltk_tokenizer_\"+str(max_word_len)+\\'words_only_cornell.pickle\\', \\'wb\\') as f:\\n        # Pickle the \\'data\\' dictionary using the highest protocol available.\\n        pickle.dump(embedding_matrix, f, pickle.HIGHEST_PROTOCOL)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QJrqv6QwynI",
        "colab_type": "code",
        "outputId": "fcfaf3af-e911-40fb-cbf6-2a18390366d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"with open(\"conversations/glove300D_all_embeddings_\"+str(max_word_len)+'words_only_cornell.pickle', 'wb') as f:\n",
        "        # Pickle the 'data' dictionary using the highest protocol available.\n",
        "        pickle.dump(embeddings_index, f, pickle.HIGHEST_PROTOCOL)\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'with open(\"conversations/glove300D_all_embeddings_\"+str(max_word_len)+\\'words_only_cornell.pickle\\', \\'wb\\') as f:\\n        # Pickle the \\'data\\' dictionary using the highest protocol available.\\n        pickle.dump(embeddings_index, f, pickle.HIGHEST_PROTOCOL)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml99He7nHABO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"conversations/glove300D_embedding_nltk_tokenizer_\"+str(max_word_len)+'words_only_cornell.pickle', 'rb') as f:\n",
        "    # The protocol version used is detected automatically, so we do not\n",
        "    # have to specify it.\n",
        "    embedding_matrix = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18LBzv7VW1Ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_layer_norm = LayerNormalization(epsilon=1e-6, name='decoder_layer_norm')\n",
        "attention_layer_norm = LayerNormalization(epsilon=1e-6, name='attention_layer_norm')\n",
        "context_layer_norm = LayerNormalization(epsilon=1e-6, name='context_layer_norm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGCSkGXqVsXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(x):\n",
        "    x = tf.argmax(x)\n",
        "    x = tf.one_hot(x, VOCAB_SIZE)\n",
        "    x = repeator3(x)\n",
        "    #x = tf.expand_dims(x, axis=1)\n",
        "    return x\n",
        "\n",
        "\"\"\"\n",
        "def which(x):\n",
        "    x = tf.argmax(x, axis=-1)\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    #x = tf.reshape(x, (1,1,1))\n",
        "    x = tf.expand_dims(x, axis=1)\n",
        "    x = tf.expand_dims(x, axis=1) \n",
        "    return x\n",
        "\"\"\"\n",
        "\n",
        "def which(x):\n",
        "    x = tf.argmax(x, axis=-1)\n",
        "    x = tf.expand_dims(x, axis=1) \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIZrRUpbxAps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def my_model(Tx, Ty, n_a, n_s, vocab_size, embedding_matrix, embed_size=300):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "    # for the decoder LSTM with shape (n_s,)\n",
        "    X1 = Input(shape=(Tx, ), name='X1')\n",
        "\n",
        "    X2 = Input(shape=(Ty, ), name='X2')\n",
        "\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    #x0 = Input(shape=(1,1,), name='x0')\n",
        "    x0 = Input(shape=(1,embed_size,), name='x0')\n",
        "\n",
        "    s = s0\n",
        "    c = c0\n",
        "    x = x0\n",
        "\n",
        "    # Initialize empty list of outputs\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    embLX1 = Embedding(vocab_size, embed_size, mask_zero=True, \n",
        "                       weights=[embedding_matrix],\n",
        "                       embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable = True)\n",
        "    embLX2 = Embedding(vocab_size, embed_size, mask_zero=True, \n",
        "                       weights=[embedding_matrix],\n",
        "                      embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable = True)\n",
        "\n",
        "    embX1 = embLX1(X1)\n",
        "    embX2 = embLX2(X2)\n",
        "    #embX2 = embLX1(X2)\n",
        "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "    enc = Bidirectional(LSTM(units=n_a, return_sequences=True), name='encoder')(embX1)\n",
        "\n",
        "    enc = LayerNormalization(epsilon=1e-6, name='encoder_layer_norm')(enc)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for t in range(Ty):\n",
        "\n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(enc, s)\n",
        "\n",
        "        #context = post_attention_dense(context) # ??? might revert emb_dim to 300 if current one is not good\n",
        "        #context = context_layer_norm(context)   # ???\n",
        "    \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "\n",
        "        #context = Lambda(lambda context: tf.reshape(context, (-1,-1, embed_size)))(context)\n",
        "        \n",
        "        embX2t = Lambda(lambda args: tf.expand_dims(args[0][:,args[1],:], axis=1))((embX2, t))\n",
        "\n",
        "        #print(\"e\",x)\n",
        "        \n",
        "        inputs = concatenator2([context, embX2t, x])\n",
        "        #inputs = concatenator2([context, embX2t])\n",
        "        #inputs = context\n",
        "\n",
        "        inputs = attention_layer_norm(inputs)\n",
        "\n",
        "        s, _, c = post_attention_LSTM_cell(inputs=inputs, initial_state=[s, c])\n",
        "\n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "\n",
        "        s = decoder_layer_norm(s)\n",
        "\n",
        "        out = output_layer(inputs=s)\n",
        "\n",
        "        #x = Lambda(which2)(out) # might use one_hot on x, then pass through Dense to get shape of embed_size\n",
        "        x = Lambda(which)(out) # might use one_hot on x, then pass through Dense to get shape of embed_size\n",
        "        x = embLX2(x)\n",
        "        # or, could pass ^out directly through Dense to get shape of embed_size     <--------- preferred\n",
        "        #x = post_output_dense(out)\n",
        "        #x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)\n",
        "\n",
        "\n",
        "        outputs.append(out)\n",
        "\n",
        "    outputs = Lambda(lambda X: tf.transpose(X, [1, 0, 2]), name='outputs')(outputs)\n",
        "\n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs=[X1,X2,s0,c0, x0], outputs=outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy4ZlUnj-kPd",
        "colab_type": "code",
        "outputId": "f886e99c-1e78-4082-fd72-c5860747dccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "VOCAB_SIZE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCFwI8pQxmII",
        "colab_type": "code",
        "outputId": "f2f485b1-fdb8-4bdb-c1a5-bac0b0fbbcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "model = my_model(Tx=max_seq_len, Ty=max_seq_len-1 , n_a=n_a, n_s=n_s, \n",
        "                 vocab_size=VOCAB_SIZE, embedding_matrix=embedding_matrix, embed_size=embed_size)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "X1 (InputLayer)                 [(None, 27)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 27, 300)      12937800    X1[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Bidirectional)         (None, 27, 300)      541200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_layer_norm (LayerNormal (None, 27, 300)      600         encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 27, 300)      0           s0[0][0]                         \n",
            "                                                                 decoder_layer_norm[0][0]         \n",
            "                                                                 decoder_layer_norm[1][0]         \n",
            "                                                                 decoder_layer_norm[2][0]         \n",
            "                                                                 decoder_layer_norm[3][0]         \n",
            "                                                                 decoder_layer_norm[4][0]         \n",
            "                                                                 decoder_layer_norm[5][0]         \n",
            "                                                                 decoder_layer_norm[6][0]         \n",
            "                                                                 decoder_layer_norm[7][0]         \n",
            "                                                                 decoder_layer_norm[8][0]         \n",
            "                                                                 decoder_layer_norm[9][0]         \n",
            "                                                                 decoder_layer_norm[10][0]        \n",
            "                                                                 decoder_layer_norm[11][0]        \n",
            "                                                                 decoder_layer_norm[12][0]        \n",
            "                                                                 decoder_layer_norm[13][0]        \n",
            "                                                                 decoder_layer_norm[14][0]        \n",
            "                                                                 decoder_layer_norm[15][0]        \n",
            "                                                                 decoder_layer_norm[16][0]        \n",
            "                                                                 decoder_layer_norm[17][0]        \n",
            "                                                                 decoder_layer_norm[18][0]        \n",
            "                                                                 decoder_layer_norm[19][0]        \n",
            "                                                                 decoder_layer_norm[20][0]        \n",
            "                                                                 decoder_layer_norm[21][0]        \n",
            "                                                                 decoder_layer_norm[22][0]        \n",
            "                                                                 decoder_layer_norm[23][0]        \n",
            "                                                                 decoder_layer_norm[24][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 27, 600)      0           encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[0][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[1][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[2][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[3][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[4][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[5][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[6][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[7][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[8][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[9][0]              \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[10][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[11][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[12][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[13][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[14][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[15][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[16][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[17][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[18][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[19][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[20][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[21][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[22][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[23][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[24][0]             \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 repeat_vector[25][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 27, 100)      60100       concatenate[0][0]                \n",
            "                                                                 concatenate[1][0]                \n",
            "                                                                 concatenate[2][0]                \n",
            "                                                                 concatenate[3][0]                \n",
            "                                                                 concatenate[4][0]                \n",
            "                                                                 concatenate[5][0]                \n",
            "                                                                 concatenate[6][0]                \n",
            "                                                                 concatenate[7][0]                \n",
            "                                                                 concatenate[8][0]                \n",
            "                                                                 concatenate[9][0]                \n",
            "                                                                 concatenate[10][0]               \n",
            "                                                                 concatenate[11][0]               \n",
            "                                                                 concatenate[12][0]               \n",
            "                                                                 concatenate[13][0]               \n",
            "                                                                 concatenate[14][0]               \n",
            "                                                                 concatenate[15][0]               \n",
            "                                                                 concatenate[16][0]               \n",
            "                                                                 concatenate[17][0]               \n",
            "                                                                 concatenate[18][0]               \n",
            "                                                                 concatenate[19][0]               \n",
            "                                                                 concatenate[20][0]               \n",
            "                                                                 concatenate[21][0]               \n",
            "                                                                 concatenate[22][0]               \n",
            "                                                                 concatenate[23][0]               \n",
            "                                                                 concatenate[24][0]               \n",
            "                                                                 concatenate[25][0]               \n",
            "__________________________________________________________________________________________________\n",
            "X2 (InputLayer)                 [(None, 26)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 27, 1)        101         dense[0][0]                      \n",
            "                                                                 dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "                                                                 dense[3][0]                      \n",
            "                                                                 dense[4][0]                      \n",
            "                                                                 dense[5][0]                      \n",
            "                                                                 dense[6][0]                      \n",
            "                                                                 dense[7][0]                      \n",
            "                                                                 dense[8][0]                      \n",
            "                                                                 dense[9][0]                      \n",
            "                                                                 dense[10][0]                     \n",
            "                                                                 dense[11][0]                     \n",
            "                                                                 dense[12][0]                     \n",
            "                                                                 dense[13][0]                     \n",
            "                                                                 dense[14][0]                     \n",
            "                                                                 dense[15][0]                     \n",
            "                                                                 dense[16][0]                     \n",
            "                                                                 dense[17][0]                     \n",
            "                                                                 dense[18][0]                     \n",
            "                                                                 dense[19][0]                     \n",
            "                                                                 dense[20][0]                     \n",
            "                                                                 dense[21][0]                     \n",
            "                                                                 dense[22][0]                     \n",
            "                                                                 dense[23][0]                     \n",
            "                                                                 dense[24][0]                     \n",
            "                                                                 dense[25][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         multiple             12937800    X2[0][0]                         \n",
            "                                                                 lambda_1[0][0]                   \n",
            "                                                                 lambda_3[0][0]                   \n",
            "                                                                 lambda_5[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 lambda_13[0][0]                  \n",
            "                                                                 lambda_15[0][0]                  \n",
            "                                                                 lambda_17[0][0]                  \n",
            "                                                                 lambda_19[0][0]                  \n",
            "                                                                 lambda_21[0][0]                  \n",
            "                                                                 lambda_23[0][0]                  \n",
            "                                                                 lambda_25[0][0]                  \n",
            "                                                                 lambda_27[0][0]                  \n",
            "                                                                 lambda_29[0][0]                  \n",
            "                                                                 lambda_31[0][0]                  \n",
            "                                                                 lambda_33[0][0]                  \n",
            "                                                                 lambda_35[0][0]                  \n",
            "                                                                 lambda_37[0][0]                  \n",
            "                                                                 lambda_39[0][0]                  \n",
            "                                                                 lambda_41[0][0]                  \n",
            "                                                                 lambda_43[0][0]                  \n",
            "                                                                 lambda_45[0][0]                  \n",
            "                                                                 lambda_47[0][0]                  \n",
            "                                                                 lambda_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 27, 1)        0           dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "                                                                 dense_1[10][0]                   \n",
            "                                                                 dense_1[11][0]                   \n",
            "                                                                 dense_1[12][0]                   \n",
            "                                                                 dense_1[13][0]                   \n",
            "                                                                 dense_1[14][0]                   \n",
            "                                                                 dense_1[15][0]                   \n",
            "                                                                 dense_1[16][0]                   \n",
            "                                                                 dense_1[17][0]                   \n",
            "                                                                 dense_1[18][0]                   \n",
            "                                                                 dense_1[19][0]                   \n",
            "                                                                 dense_1[20][0]                   \n",
            "                                                                 dense_1[21][0]                   \n",
            "                                                                 dense_1[22][0]                   \n",
            "                                                                 dense_1[23][0]                   \n",
            "                                                                 dense_1[24][0]                   \n",
            "                                                                 dense_1[25][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1, 300)       0           attention_weights[0][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[10][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[11][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[12][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[13][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[14][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[15][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[16][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[17][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[18][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[19][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[20][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[21][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[22][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[23][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[24][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "                                                                 attention_weights[25][0]         \n",
            "                                                                 encoder_layer_norm[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(None, 1, 300)]     0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "x0 (InputLayer)                 [(None, 1, 300)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1, 900)       0           dot[0][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims[0][0]     \n",
            "                                                                 x0[0][0]                         \n",
            "                                                                 dot[1][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_1[0][0]   \n",
            "                                                                 embedding_1[1][0]                \n",
            "                                                                 dot[2][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_2[0][0]   \n",
            "                                                                 embedding_1[2][0]                \n",
            "                                                                 dot[3][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_3[0][0]   \n",
            "                                                                 embedding_1[3][0]                \n",
            "                                                                 dot[4][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_4[0][0]   \n",
            "                                                                 embedding_1[4][0]                \n",
            "                                                                 dot[5][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_5[0][0]   \n",
            "                                                                 embedding_1[5][0]                \n",
            "                                                                 dot[6][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_6[0][0]   \n",
            "                                                                 embedding_1[6][0]                \n",
            "                                                                 dot[7][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_7[0][0]   \n",
            "                                                                 embedding_1[7][0]                \n",
            "                                                                 dot[8][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_8[0][0]   \n",
            "                                                                 embedding_1[8][0]                \n",
            "                                                                 dot[9][0]                        \n",
            "                                                                 tf_op_layer_ExpandDims_9[0][0]   \n",
            "                                                                 embedding_1[9][0]                \n",
            "                                                                 dot[10][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_10[0][0]  \n",
            "                                                                 embedding_1[10][0]               \n",
            "                                                                 dot[11][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_11[0][0]  \n",
            "                                                                 embedding_1[11][0]               \n",
            "                                                                 dot[12][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_12[0][0]  \n",
            "                                                                 embedding_1[12][0]               \n",
            "                                                                 dot[13][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_13[0][0]  \n",
            "                                                                 embedding_1[13][0]               \n",
            "                                                                 dot[14][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_14[0][0]  \n",
            "                                                                 embedding_1[14][0]               \n",
            "                                                                 dot[15][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_15[0][0]  \n",
            "                                                                 embedding_1[15][0]               \n",
            "                                                                 dot[16][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_16[0][0]  \n",
            "                                                                 embedding_1[16][0]               \n",
            "                                                                 dot[17][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_17[0][0]  \n",
            "                                                                 embedding_1[17][0]               \n",
            "                                                                 dot[18][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_18[0][0]  \n",
            "                                                                 embedding_1[18][0]               \n",
            "                                                                 dot[19][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_19[0][0]  \n",
            "                                                                 embedding_1[19][0]               \n",
            "                                                                 dot[20][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_20[0][0]  \n",
            "                                                                 embedding_1[20][0]               \n",
            "                                                                 dot[21][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_21[0][0]  \n",
            "                                                                 embedding_1[21][0]               \n",
            "                                                                 dot[22][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_22[0][0]  \n",
            "                                                                 embedding_1[22][0]               \n",
            "                                                                 dot[23][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_23[0][0]  \n",
            "                                                                 embedding_1[23][0]               \n",
            "                                                                 dot[24][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_24[0][0]  \n",
            "                                                                 embedding_1[24][0]               \n",
            "                                                                 dot[25][0]                       \n",
            "                                                                 tf_op_layer_ExpandDims_25[0][0]  \n",
            "                                                                 embedding_1[25][0]               \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer_norm (LayerNorm (None, 1, 900)       1800        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "                                                                 concatenate_1[10][0]             \n",
            "                                                                 concatenate_1[11][0]             \n",
            "                                                                 concatenate_1[12][0]             \n",
            "                                                                 concatenate_1[13][0]             \n",
            "                                                                 concatenate_1[14][0]             \n",
            "                                                                 concatenate_1[15][0]             \n",
            "                                                                 concatenate_1[16][0]             \n",
            "                                                                 concatenate_1[17][0]             \n",
            "                                                                 concatenate_1[18][0]             \n",
            "                                                                 concatenate_1[19][0]             \n",
            "                                                                 concatenate_1[20][0]             \n",
            "                                                                 concatenate_1[21][0]             \n",
            "                                                                 concatenate_1[22][0]             \n",
            "                                                                 concatenate_1[23][0]             \n",
            "                                                                 concatenate_1[24][0]             \n",
            "                                                                 concatenate_1[25][0]             \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 [(None, 300)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder (LSTM)                  [(None, 300), (None, 1441200     attention_layer_norm[0][0]       \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 attention_layer_norm[1][0]       \n",
            "                                                                 decoder_layer_norm[0][0]         \n",
            "                                                                 decoder[0][2]                    \n",
            "                                                                 attention_layer_norm[2][0]       \n",
            "                                                                 decoder_layer_norm[1][0]         \n",
            "                                                                 decoder[1][2]                    \n",
            "                                                                 attention_layer_norm[3][0]       \n",
            "                                                                 decoder_layer_norm[2][0]         \n",
            "                                                                 decoder[2][2]                    \n",
            "                                                                 attention_layer_norm[4][0]       \n",
            "                                                                 decoder_layer_norm[3][0]         \n",
            "                                                                 decoder[3][2]                    \n",
            "                                                                 attention_layer_norm[5][0]       \n",
            "                                                                 decoder_layer_norm[4][0]         \n",
            "                                                                 decoder[4][2]                    \n",
            "                                                                 attention_layer_norm[6][0]       \n",
            "                                                                 decoder_layer_norm[5][0]         \n",
            "                                                                 decoder[5][2]                    \n",
            "                                                                 attention_layer_norm[7][0]       \n",
            "                                                                 decoder_layer_norm[6][0]         \n",
            "                                                                 decoder[6][2]                    \n",
            "                                                                 attention_layer_norm[8][0]       \n",
            "                                                                 decoder_layer_norm[7][0]         \n",
            "                                                                 decoder[7][2]                    \n",
            "                                                                 attention_layer_norm[9][0]       \n",
            "                                                                 decoder_layer_norm[8][0]         \n",
            "                                                                 decoder[8][2]                    \n",
            "                                                                 attention_layer_norm[10][0]      \n",
            "                                                                 decoder_layer_norm[9][0]         \n",
            "                                                                 decoder[9][2]                    \n",
            "                                                                 attention_layer_norm[11][0]      \n",
            "                                                                 decoder_layer_norm[10][0]        \n",
            "                                                                 decoder[10][2]                   \n",
            "                                                                 attention_layer_norm[12][0]      \n",
            "                                                                 decoder_layer_norm[11][0]        \n",
            "                                                                 decoder[11][2]                   \n",
            "                                                                 attention_layer_norm[13][0]      \n",
            "                                                                 decoder_layer_norm[12][0]        \n",
            "                                                                 decoder[12][2]                   \n",
            "                                                                 attention_layer_norm[14][0]      \n",
            "                                                                 decoder_layer_norm[13][0]        \n",
            "                                                                 decoder[13][2]                   \n",
            "                                                                 attention_layer_norm[15][0]      \n",
            "                                                                 decoder_layer_norm[14][0]        \n",
            "                                                                 decoder[14][2]                   \n",
            "                                                                 attention_layer_norm[16][0]      \n",
            "                                                                 decoder_layer_norm[15][0]        \n",
            "                                                                 decoder[15][2]                   \n",
            "                                                                 attention_layer_norm[17][0]      \n",
            "                                                                 decoder_layer_norm[16][0]        \n",
            "                                                                 decoder[16][2]                   \n",
            "                                                                 attention_layer_norm[18][0]      \n",
            "                                                                 decoder_layer_norm[17][0]        \n",
            "                                                                 decoder[17][2]                   \n",
            "                                                                 attention_layer_norm[19][0]      \n",
            "                                                                 decoder_layer_norm[18][0]        \n",
            "                                                                 decoder[18][2]                   \n",
            "                                                                 attention_layer_norm[20][0]      \n",
            "                                                                 decoder_layer_norm[19][0]        \n",
            "                                                                 decoder[19][2]                   \n",
            "                                                                 attention_layer_norm[21][0]      \n",
            "                                                                 decoder_layer_norm[20][0]        \n",
            "                                                                 decoder[20][2]                   \n",
            "                                                                 attention_layer_norm[22][0]      \n",
            "                                                                 decoder_layer_norm[21][0]        \n",
            "                                                                 decoder[21][2]                   \n",
            "                                                                 attention_layer_norm[23][0]      \n",
            "                                                                 decoder_layer_norm[22][0]        \n",
            "                                                                 decoder[22][2]                   \n",
            "                                                                 attention_layer_norm[24][0]      \n",
            "                                                                 decoder_layer_norm[23][0]        \n",
            "                                                                 decoder[23][2]                   \n",
            "                                                                 attention_layer_norm[25][0]      \n",
            "                                                                 decoder_layer_norm[24][0]        \n",
            "                                                                 decoder[24][2]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_layer_norm (LayerNormal (None, 300)          600         decoder[0][0]                    \n",
            "                                                                 decoder[1][0]                    \n",
            "                                                                 decoder[2][0]                    \n",
            "                                                                 decoder[3][0]                    \n",
            "                                                                 decoder[4][0]                    \n",
            "                                                                 decoder[5][0]                    \n",
            "                                                                 decoder[6][0]                    \n",
            "                                                                 decoder[7][0]                    \n",
            "                                                                 decoder[8][0]                    \n",
            "                                                                 decoder[9][0]                    \n",
            "                                                                 decoder[10][0]                   \n",
            "                                                                 decoder[11][0]                   \n",
            "                                                                 decoder[12][0]                   \n",
            "                                                                 decoder[13][0]                   \n",
            "                                                                 decoder[14][0]                   \n",
            "                                                                 decoder[15][0]                   \n",
            "                                                                 decoder[16][0]                   \n",
            "                                                                 decoder[17][0]                   \n",
            "                                                                 decoder[18][0]                   \n",
            "                                                                 decoder[19][0]                   \n",
            "                                                                 decoder[20][0]                   \n",
            "                                                                 decoder[21][0]                   \n",
            "                                                                 decoder[22][0]                   \n",
            "                                                                 decoder[23][0]                   \n",
            "                                                                 decoder[24][0]                   \n",
            "                                                                 decoder[25][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 43126)        12980926    decoder_layer_norm[0][0]         \n",
            "                                                                 decoder_layer_norm[1][0]         \n",
            "                                                                 decoder_layer_norm[2][0]         \n",
            "                                                                 decoder_layer_norm[3][0]         \n",
            "                                                                 decoder_layer_norm[4][0]         \n",
            "                                                                 decoder_layer_norm[5][0]         \n",
            "                                                                 decoder_layer_norm[6][0]         \n",
            "                                                                 decoder_layer_norm[7][0]         \n",
            "                                                                 decoder_layer_norm[8][0]         \n",
            "                                                                 decoder_layer_norm[9][0]         \n",
            "                                                                 decoder_layer_norm[10][0]        \n",
            "                                                                 decoder_layer_norm[11][0]        \n",
            "                                                                 decoder_layer_norm[12][0]        \n",
            "                                                                 decoder_layer_norm[13][0]        \n",
            "                                                                 decoder_layer_norm[14][0]        \n",
            "                                                                 decoder_layer_norm[15][0]        \n",
            "                                                                 decoder_layer_norm[16][0]        \n",
            "                                                                 decoder_layer_norm[17][0]        \n",
            "                                                                 decoder_layer_norm[18][0]        \n",
            "                                                                 decoder_layer_norm[19][0]        \n",
            "                                                                 decoder_layer_norm[20][0]        \n",
            "                                                                 decoder_layer_norm[21][0]        \n",
            "                                                                 decoder_layer_norm[22][0]        \n",
            "                                                                 decoder_layer_norm[23][0]        \n",
            "                                                                 decoder_layer_norm[24][0]        \n",
            "                                                                 decoder_layer_norm[25][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_1 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 1)            0           dense_3[1][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_3 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 1)            0           dense_3[2][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_4 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 1)            0           dense_3[3][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_4 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_5 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 1)            0           dense_3[4][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_5 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_5[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_6 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 1)            0           dense_3[5][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_6 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_6[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_7 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, 1)            0           dense_3[6][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_7 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_7[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_8 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 1)            0           dense_3[7][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_8 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_8[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_9 (Te [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, 1)            0           dense_3[8][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_9 (Tenso [(None, 1, 300)]     0           tf_op_layer_strided_slice_9[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_10 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_19 (Lambda)              (None, 1)            0           dense_3[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_10 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_10[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_11 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_21 (Lambda)              (None, 1)            0           dense_3[10][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_11 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_11[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_12 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_23 (Lambda)              (None, 1)            0           dense_3[11][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_12 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_12[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_13 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_25 (Lambda)              (None, 1)            0           dense_3[12][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_13 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_13[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_14 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_27 (Lambda)              (None, 1)            0           dense_3[13][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_14 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_14[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_15 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_29 (Lambda)              (None, 1)            0           dense_3[14][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_15 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_15[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_16 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_31 (Lambda)              (None, 1)            0           dense_3[15][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_16 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_16[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_17 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 1)            0           dense_3[16][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_17 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_17[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_18 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 1)            0           dense_3[17][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_18 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_18[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_19 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 1)            0           dense_3[18][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_19 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_19[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_20 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 1)            0           dense_3[19][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_20 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_20[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_21 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_41 (Lambda)              (None, 1)            0           dense_3[20][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_21 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_21[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_22 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_43 (Lambda)              (None, 1)            0           dense_3[21][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_22 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_22[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_23 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_45 (Lambda)              (None, 1)            0           dense_3[22][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_23 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_23[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_24 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_47 (Lambda)              (None, 1)            0           dense_3[23][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_24 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_24[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_25 (T [(None, 300)]        0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_49 (Lambda)              (None, 1)            0           dense_3[24][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_25 (Tens [(None, 1, 300)]     0           tf_op_layer_strided_slice_25[0][0\n",
            "__________________________________________________________________________________________________\n",
            "outputs (Lambda)                (None, 26, 43126)    0           dense_3[0][0]                    \n",
            "                                                                 dense_3[1][0]                    \n",
            "                                                                 dense_3[2][0]                    \n",
            "                                                                 dense_3[3][0]                    \n",
            "                                                                 dense_3[4][0]                    \n",
            "                                                                 dense_3[5][0]                    \n",
            "                                                                 dense_3[6][0]                    \n",
            "                                                                 dense_3[7][0]                    \n",
            "                                                                 dense_3[8][0]                    \n",
            "                                                                 dense_3[9][0]                    \n",
            "                                                                 dense_3[10][0]                   \n",
            "                                                                 dense_3[11][0]                   \n",
            "                                                                 dense_3[12][0]                   \n",
            "                                                                 dense_3[13][0]                   \n",
            "                                                                 dense_3[14][0]                   \n",
            "                                                                 dense_3[15][0]                   \n",
            "                                                                 dense_3[16][0]                   \n",
            "                                                                 dense_3[17][0]                   \n",
            "                                                                 dense_3[18][0]                   \n",
            "                                                                 dense_3[19][0]                   \n",
            "                                                                 dense_3[20][0]                   \n",
            "                                                                 dense_3[21][0]                   \n",
            "                                                                 dense_3[22][0]                   \n",
            "                                                                 dense_3[23][0]                   \n",
            "                                                                 dense_3[24][0]                   \n",
            "                                                                 dense_3[25][0]                   \n",
            "==================================================================================================\n",
            "Total params: 40,902,127\n",
            "Trainable params: 40,902,127\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21V-LHhmyuMG",
        "colab_type": "code",
        "outputId": "e55db2bc-784d-43c1-9be5-58f0a28b9e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"### START CODE HERE ### (≈2 lines)\n",
        "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "### END CODE HERE ###\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### START CODE HERE ### (≈2 lines)\\nopt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\\nmodel.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\\n### END CODE HERE ###'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiiK9-9DJwhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, max_seq_len - 1))\n",
        "  \n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # ensure labels have shape (batch_size, max_seq_len - 1)\n",
        "  y_true = tf.reshape(y_true, shape=(-1, max_seq_len - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsdzPT5znh08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7M_Szr0yEl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(embed_size)\n",
        "\n",
        "opt = Adam()#learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "\n",
        "loss_weights = list(np.zeros(Ty))\n",
        "\n",
        "for i in range(Ty):\n",
        "    loss_weights[i] = (Ty-i)/Ty\n",
        "    #loss_weights[i] = 1.0/(i+1)\n",
        "\n",
        "model.compile(optimizer=opt, loss=loss_function, metrics=[accuracy])#, loss_weights=loss_weights)\n",
        "\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEKYwLW6b5d6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_mini_batches(X1, X2, Y, mini_batch_size = 64):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(911-786)            # To make your \"random\" minibatches the same as ours\n",
        "    m = X1.shape[0]                  # number of training examples\n",
        "    mini_batches = []\n",
        "\n",
        "    n_y = Y.shape[0]\n",
        "        \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X1 = X1[permutation]\n",
        "    shuffled_X2 = X2[permutation]\n",
        "    shuffled_Y = Y[permutation]\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        mini_batch_X1 = shuffled_X1[k*mini_batch_size : (k+1) * mini_batch_size]\n",
        "        mini_batch_X2 = shuffled_X2[k*mini_batch_size : (k+1) * mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[k*mini_batch_size : (k+1) * mini_batch_size]\n",
        "        ### END CODE HERE ###\n",
        "        mini_batch = (mini_batch_X1, mini_batch_X2, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        mini_batch_X1 = shuffled_X1[num_complete_minibatches*mini_batch_size : ]\n",
        "        mini_batch_X2 = shuffled_X2[num_complete_minibatches*mini_batch_size : ]\n",
        "        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size : ]\n",
        "        ### END CODE HERE ###\n",
        "        mini_batch = (mini_batch_X1, mini_batch_X2, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOA1u5XNvCKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X1 = X_train\n",
        "X2 = y_train[:,:-1]\n",
        "Y = y_train[:, 1:]\n",
        "m = X1.shape[0]\n",
        "\n",
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "#x0 = np.ones((m, 1, 1))*START_TOKEN\n",
        "x0 = np.zeros((m, 1, embed_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-hZSO2HZZUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 200000\n",
        "# decoder inputs use the previous target as input\n",
        "# remove START_TOKEN from targets\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'X1': X1,\n",
        "        'X2': X2,\n",
        "        's0': s0,\n",
        "        'c0': c0,\n",
        "        'x0': x0\n",
        "    },\n",
        "    {\n",
        "        'outputs': Y\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPbUCjGyOkqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQArt_YSbFfp",
        "colab_type": "code",
        "outputId": "84b794e8-7e92-41be-8a22-32ae821bb4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"#model.load_weights('saved_word_emb3_weights.h5')\n",
        "\n",
        "for i in range(10):\n",
        "    model.fit(dataset, #validation_data=validation_data,\n",
        "                    epochs=1, \n",
        "                    shuffle=True, #batch_size=64, \n",
        "                    verbose=1)\n",
        "\n",
        "    model.save('my_word_emb3_model.h5')\n",
        "    model.save_weights('saved_word_emb3_weights.h5')\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#model.load_weights('saved_word_emb3_weights.h5')\\n\\nfor i in range(10):\\n    model.fit(dataset, #validation_data=validation_data,\\n                    epochs=1, \\n                    shuffle=True, #batch_size=64, \\n                    verbose=1)\\n\\n    model.save('my_word_emb3_model.h5')\\n    model.save_weights('saved_word_emb3_weights.h5')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWrbQPkgZR1q",
        "colab_type": "code",
        "outputId": "e2e9d089-f84d-4c7c-c832-9e5ccef6bca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model.load_weights('saved_word_emb3_weights.h5')\n",
        "\n",
        "for i in range(10):\n",
        "    model.fit([X1, X2, s0, c0, x0], Y, #validation_data=validation_data,\n",
        "                    epochs=1, shuffle=True, batch_size=64, \n",
        "                    verbose=1)\n",
        "\n",
        "    model.save('my_word_emb3_model.h5')\n",
        "    model.save_weights('saved_word_emb3_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2645/2645 [==============================] - 1943s 735ms/step - loss: 4.2225 - accuracy: 0.0743\n",
            "2645/2645 [==============================] - 1977s 747ms/step - loss: 4.2149 - accuracy: 0.0812\n",
            "2645/2645 [==============================] - 1988s 752ms/step - loss: 4.2144 - accuracy: 0.0817\n",
            "  19/2645 [..............................] - ETA: 33:52 - loss: 4.1108 - accuracy: 0.0799"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgMaeQkhbDOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRehEahsy_Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"minibatch_size = 1024\n",
        "\n",
        "#model = load_model('my_word_emb2_model.h5')\n",
        "#model.load_weights('saved_word_emb2_weights.h5')\n",
        "y_train = y_train[:,1:]\n",
        "minibatches = random_mini_batches(X_train, y_train, mini_batch_size = minibatch_size)\n",
        "del X_train\n",
        "del y_train\n",
        "\n",
        "for i in range(100):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    num_minibatches = len(minibatches)\n",
        "\n",
        "    j = 0\n",
        "\n",
        "    for minibatch in minibatches:\n",
        "        (minibatch_X, minibatch_Y) = minibatch\n",
        "\n",
        "        #Yoh_train = get_one_hot_seqs(minibatch_Y, VOCAB_SIZE)\n",
        "\n",
        "        #outputs = list(Yoh_train.swapaxes(0,1))\n",
        "\n",
        "        m = minibatch_X.shape[0]\n",
        "        #valshape = X_val.shape[0]\n",
        "        #s0 = np.zeros((m, n_s))\n",
        "        #c0 = np.zeros((m, n_s))\n",
        "        #x0 = np.ones((m, 1, 1))*START_TOKEN\n",
        "        #x0 = np.zeros((m, 1, VOCAB_SIZE))\n",
        "\n",
        "        if j%10 == 0 or True:\n",
        "            #print(\"current loss:\", epoch_loss)\n",
        "            history = model.fit([minibatch_X, np.zeros((m, n_s)), np.zeros((m, n_s)), np.ones((m, 1, 1))*START_TOKEN], \n",
        "                                list(get_one_hot_seqs(minibatch_Y, VOCAB_SIZE).swapaxes(0,1)), #validation_data=validation_data,\n",
        "                                #list(minibatch_Y.swapaxes(0,1)),\n",
        "                                epochs=1,\n",
        "                                shuffle=True, \n",
        "                                batch_size=128,\n",
        "                                verbose=1\n",
        "                                )\n",
        "            model.save('my_word_emb2_model.h5')\n",
        "            model.save_weights('saved_word_emb2_weights.h5')\n",
        "        else:\n",
        "            history = model.fit([minibatch_X, np.zeros((m, n_s)), np.zeros((m, n_s)), np.ones((m, 1, 1))*START_TOKEN], \n",
        "                                list(get_one_hot_seqs(minibatch_Y, VOCAB_SIZE).swapaxes(0,1)), #validation_data=validation_data,\n",
        "                                #list(minibatch_Y.swapaxes(0,1)),          \n",
        "                                epochs=1,\n",
        "                                shuffle=True, \n",
        "                                batch_size=128,\n",
        "                                verbose=0\n",
        "                                )\n",
        "                            \n",
        "        epoch_loss += history.history['loss'][-1]\n",
        "\n",
        "        j += 1\n",
        "\n",
        "        #del Xoh_train\n",
        "        #del Yoh_train\n",
        "        #del outputs\n",
        "        del minibatch\n",
        "        del minibatch_X\n",
        "        del minibatch_Y\n",
        "        #del s0\n",
        "        #del c0\n",
        "        #del x0  \n",
        "\n",
        "    print(\"loss after epoch\", i, \" is\", epoch_loss)\n",
        "    model.save('my_word_emb2_model.h5')\n",
        "    model.save_weights('saved_word_emb2_weights.h5')  \n",
        "\n",
        "    del minibatches\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LawdPo0y3pL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "m = Sequential()\n",
        "m.add(Embedding(VOCAB_SIZE, embed_size, mask_zero=True, weights=[embedding_matrix],\n",
        "                       embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable = True))\n",
        "\n",
        "input_array = tf.expand_dims([word_index['hello']],0)\n",
        "\n",
        "#input_array = tf.expand_dims([START_TOKEN]+[END_TOKEN],0)\n",
        "\n",
        "m.compile('adam', 'accuracy')\n",
        "out = m.predict(input_array, steps=1)\n",
        "\n",
        "print(out)\n",
        "\n",
        "print(embedding_matrix[word_index['hello']])\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH0-ra5v4EmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}